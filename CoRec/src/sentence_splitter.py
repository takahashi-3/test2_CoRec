import numpy as np
import csv
import joblib
import torch

import config
import dataset
import engine
from train import process_data
from model import EntityModel


def get_chunk_type(tag_name):
    tag_class = tag_name.split('-')[0]
    tag_type = tag_name.split('-')[-1]
    return tag_class, tag_type


def get_chunks(seq):
    default = "O"
    chunks = []
    chunk_type, chunk_start = None, None
    for i, tok in enumerate(seq):
        # only end of a chunk
        if tok == default and chunk_type is not None:
            # add a chunk.
            chunk = (chunk_type, chunk_start, i)
            if chunk_type != "C":
                chunks.append(chunk)
            chunk_type, chunk_start = None, None

        # end of a chunk + start of another chunk!
        elif tok != default:
            tok_chunk_class, tok_chunk_type = get_chunk_type(tok)
            if chunk_type is None:
                chunk_type, chunk_start = tok_chunk_type, i

            elif tok_chunk_type != chunk_type or tok_chunk_class == "B":
                chunk = (chunk_type, chunk_start, i)
                if chunk_type != "C":
                    chunks.append(chunk)
                chunk_type, chunk_start = tok_chunk_type, i
        else:
            pass

    # end condition
    if chunk_type is not None:
        chunk = (chunk_type, chunk_start, len(seq))
        chunks.append(chunk)

    return chunks


# eliminate the sub-words
def getCleanLabels(raw_tokens, raw_tags):
    tokens, tags = [], []
    tmp_str = ""
    tmp_tag = ""
    for i, t in enumerate(raw_tokens):
        if i == 0:
            # the first and last tokens are [CLS] and [SEP], should be skipped
            continue
        else:
            if raw_tokens[i].startswith("##"):
                tmp_str += raw_tokens[i].lstrip("##")
            elif raw_tokens[i] == 'c':
                if tmp_str == '[':
                    tmp_str += raw_tokens[i]
                else:
                    if tmp_str != "":
                        tokens.append(tmp_str)
                        tags.append(tmp_tag)
                    tmp_str = raw_tokens[i]
                    tmp_tag = raw_tags[i]
            elif raw_tokens[i] == ']':
                if tmp_str == '[c':
                    tmp_str += raw_tokens[i]
                else:
                    if tmp_str != "":
                        tokens.append(tmp_str)
                        tags.append(tmp_tag)
                    tmp_str = raw_tokens[i]
                    tmp_tag = raw_tags[i]
            else:
                if tmp_str != "":
                    tokens.append(tmp_str)
                    tags.append(tmp_tag)
                if raw_tokens[i] == "[SEP]":
                    break
                tmp_str = raw_tokens[i]
                tmp_tag = raw_tags[i]

    return tokens, tags



def splitter(tokens, ctags, pred_tags):
    finished_subs = []
    unfinished_subs = []
    unfinished_ctags = []

    bg_tokens = []
    bg_ctags = []
    start_id = -1
    end_id = -1
    #beginning part
    for id in range (len(tokens)):
        if pred_tags[id] != 'O':
            start_id = id
            break
        bg_tokens.append(tokens[id])
        bg_ctags.append(ctags[id])

    conjuncts = []
    cj_ctags = []
    tmp_tokens = []
    tmp_ctags = []
    #conjuncts
    for i2 in range(start_id, len(tokens)):
        if i2 == start_id:
            if pred_tags[i2] != 'C':
                tmp_tokens.append(tokens[i2])
                tmp_ctags.append(ctags[i2])
            else:
                #ignore those 'first' instances generated by the paired coordinators
                if tokens[i2] == '[c]':
                    return [], [], []
        else:
            if pred_tags[i2] == 'B-before':
                if len(tmp_tokens) > 0:
                    conjuncts.append(tmp_tokens)
                    cj_ctags.append(tmp_ctags)
                    tmp_tokens = []
                    tmp_ctags = []
                tmp_tokens.append(tokens[i2])
                tmp_ctags.append(ctags[i2])
            elif pred_tags[i2] == 'C':
                if len(tmp_tokens) > 0:
                    conjuncts.append(tmp_tokens)
                    cj_ctags.append(tmp_ctags)
                    tmp_tokens = []
                    tmp_ctags = []
            elif pred_tags[i2] == 'O':
                if len(tmp_tokens) > 0:
                    conjuncts.append(tmp_tokens)
                    cj_ctags.append(tmp_ctags)
                end_id = i2
                break
            else:
                tmp_tokens.append(tokens[i2])
                tmp_ctags.append(ctags[i2])

    end_tokens = []
    end_ctags = []
    #ending
    for i3 in range(end_id, len(tokens)):
        end_tokens.append(tokens[i3])
        end_ctags.append(ctags[i3])

    #generate sub-sentences
    finished = True
    tmp_sub = []
    tmp_c = []
    for n in range(len(conjuncts)):
        cur_conjunct = conjuncts[n]
        cur_ctags = cj_ctags[n]
        for t1 in range(len(bg_tokens)):
            tmp_sub.append(bg_tokens[t1])
            tmp_c.append(bg_ctags[t1])
            if bg_ctags[t1] == 'c-C':
                finished = False

        for t2 in range(len(cur_conjunct)):
            tmp_sub.append(cur_conjunct[t2])
            tmp_c.append(cur_ctags[t2])
            if cur_ctags[t2] == 'c-C':
                finished = False

        for t3 in range(len(end_tokens)):
            tmp_sub.append(end_tokens[t3])
            tmp_c.append(end_ctags[t3])
            if end_ctags[t3] == 'c-C':
                finished = False

        if finished == True:
            finished_subs.append(tmp_sub)
        else:
            unfinished_subs.append(tmp_sub)
            unfinished_ctags.append(tmp_c)

        finished = True
        tmp_sub = []
        tmp_c = []

    return finished_subs, unfinished_subs, unfinished_ctags




if __name__ == "__main__":

    #test_file = "../data/wsj/wsj_test_all.csv"
    test_file = "../splitted/wsj_test_all_4.csv"

    fieldnames = ['Sentence #', 'Text', 'c-Tag', 'Tag']
    outF1 = open("../splitted/wsj_test_all_f.txt", "a")

    outF2 = open("../splitted/wsj_test_all_5.csv", "a")
    writer = csv.DictWriter(outF2, fieldnames=fieldnames)
    writer.writeheader()

    test_model_path = "saved_models/model4_2.bin"

    meta_data = joblib.load("saved_models/meta_2.bin")
    enc_train_ctag = meta_data["enc_train_ctag"]
    enc_train_tag = meta_data["enc_train_tag"]

    num_ctag = len(list(enc_train_ctag.classes_))
    num_tag = len(list(enc_train_tag.classes_))

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = EntityModel(num_ctag=num_ctag, num_tag=num_tag)
    model.load_state_dict(torch.load(test_model_path))
    model.to(device)

    test_sentences, test_ctag, test_tag, enc_test_ctag, enc_test_tag = process_data(test_file)
    test_dataset = dataset.EntityDataset(
        texts=test_sentences, ctags=test_ctag, tags=test_tag
    )


    correct_preds = 0
    total_preds = 0
    total_correct = 0

    f_cnt = 1
    uf_cnt = 1
    with torch.no_grad():
        for j in range(len(test_dataset)):
            try:
                data = test_dataset[j]
                tokenized_sentence_ids = data["ids"]
                c_tags = data["target_ctag"]
                target_tags = data["target_tag"]
                tokenized_sentence = config.TOKENIZER.convert_ids_to_tokens(tokenized_sentence_ids)

                for k, v in data.items():
                    data[k] = v.to(device).unsqueeze(0)
                tag, _ = model(**data)

                #raw_tags = enc_train_tag.inverse_transform(tag.argmax(2).cpu().numpy().reshape(-1))[:len(tokenized_sentence_ids)]
                raw_tags = enc_train_tag.inverse_transform(tag[0])[:len(tokenized_sentence_ids)]
                #print(raw_tags)
                raw_target_ctags = enc_train_ctag.inverse_transform(c_tags)[
                                  :len(tokenized_sentence_ids)]
                raw_target_tags = enc_train_tag.inverse_transform(target_tags)[
                       :len(tokenized_sentence_ids)]


                tokens, labels = getCleanLabels(tokenized_sentence, raw_tags)
                _, target_clabels = getCleanLabels(tokenized_sentence, raw_target_ctags)

                f_subs, uf_subs, uf_ctags = splitter(tokens, target_clabels, labels)

                #print(f_subs)
                #print(uf_subs)

                #finished sub-sentences
                for sub in f_subs:
                    for m in range(len(sub)):
                        outF1.write(sub[m])
                        outF1.write(" ")
                    outF1.write("\n")
                    f_cnt += 1

                #unfinished sub-sentences
                for k in range(len(uf_subs)):
                    sstr = "Sentence: " + str(uf_cnt)
                    fst = True
                    marking = False
                    ufsub = uf_subs[k]
                    ufctag = uf_ctags[k]
                    for m2 in range(len(ufsub)):
                        if ufctag[m2] == 'c-C':
                            if fst is True:
                                fst = False
                                marking = True
                                writer.writerow({'Sentence #': sstr, 'Text': '[C]', 'c-Tag': 'c-C', 'Tag': 'C'})
                        else:
                            if marking is True:
                                marking = False
                                writer.writerow({'Sentence #': sstr, 'Text': '[C]', 'c-Tag': 'c-C', 'Tag': 'C'})

                        writer.writerow({'Sentence #': sstr, 'Text': ufsub[m2], 'c-Tag': ufctag[m2], 'Tag': 'O'})

                    uf_cnt += 1

            except Exception:
                pass


    outF1.close()
    outF2.close()

    print("f_cnt:")
    print(f_cnt)
    print("uf_cnt:")
    print(uf_cnt)
