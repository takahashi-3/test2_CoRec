import os
import csv
import joblib
import torch
import config
import dataset
#from train import process_data
from model import EntityModel
import nltk
from sklearn import preprocessing
import pandas as pd


def get_chunk_type(tag_name):
    tag_class = tag_name.split('-')[0]
    tag_type = tag_name.split('-')[-1]
    return tag_class, tag_type


def get_chunks(seq):
    default = "O"
    chunks = []
    chunk_type, chunk_start = None, None
    for i, tok in enumerate(seq):
        # only end of a chunk
        if tok == default and chunk_type is not None:
            # add a chunk.
            chunk = (chunk_type, chunk_start, i)
            if chunk_type != "C":
                chunks.append(chunk)
            chunk_type, chunk_start = None, None

        # end of a chunk + start of another chunk!
        elif tok != default:
            tok_chunk_class, tok_chunk_type = get_chunk_type(tok)
            if chunk_type is None:
                chunk_type, chunk_start = tok_chunk_type, i

            elif tok_chunk_type != chunk_type or tok_chunk_class == "B":
                chunk = (chunk_type, chunk_start, i)
                if chunk_type != "C":
                    chunks.append(chunk)
                chunk_type, chunk_start = tok_chunk_type, i
        else:
            pass

    # end condition
    if chunk_type is not None:
        chunk = (chunk_type, chunk_start, len(seq))
        chunks.append(chunk)

    return chunks


# eliminate the sub-words
def getCleanLabels(raw_tokens, raw_tags):
    tokens, tags = [], []
    tmp_str = ""
    tmp_tag = ""
    for i, t in enumerate(raw_tokens):
        if i == 0:
            # the first and last tokens are [CLS] and [SEP], should be skipped
            continue
        else:
            if raw_tokens[i].startswith("##"):
                tmp_str += raw_tokens[i].lstrip("##")
            elif raw_tokens[i] == 'c':
                if tmp_str == '[':
                    tmp_str += raw_tokens[i]
                else:
                    if tmp_str != "":
                        tokens.append(tmp_str)
                        tags.append(tmp_tag)
                    tmp_str = raw_tokens[i]
                    tmp_tag = raw_tags[i]
            elif raw_tokens[i] == ']':
                if tmp_str == '[c':
                    tmp_str += raw_tokens[i]
                else:
                    if tmp_str != "":
                        tokens.append(tmp_str)
                        tags.append(tmp_tag)
                    tmp_str = raw_tokens[i]
                    tmp_tag = raw_tags[i]
            else:
                if tmp_str != "":
                    tokens.append(tmp_str)
                    tags.append(tmp_tag)
                if raw_tokens[i] == "[SEP]":
                    break
                tmp_str = raw_tokens[i]
                tmp_tag = raw_tags[i]

    return tokens, tags



def splitter(tokens, ctags, pred_tags, offsets):
    finished_subs = []
    finished_offsets = []
    unfinished_subs = []
    unfinished_ctags = []
    unfinished_offsets = []

    bg_tokens = []
    bg_ctags = []
    bg_offsets = []
    start_id = -1
    end_id = -1
    #beginning part
    for id in range (len(tokens)):
        if pred_tags[id] != 'O':
            start_id = id
            break
        bg_tokens.append(tokens[id])
        bg_ctags.append(ctags[id])
        bg_offsets.append(offsets[id])

    conjuncts = []
    cj_ctags = []
    cj_offsets = []
    tmp_tokens = []
    tmp_ctags = []
    tmp_offsets = []
    #conjuncts
    for i2 in range(start_id, len(tokens)):
        if i2 == start_id:
            if pred_tags[i2] != 'C':
                tmp_tokens.append(tokens[i2])
                tmp_ctags.append(ctags[i2])
                tmp_offsets.append(offsets[i2])
            else:
                #ignore those 'first' instances generated by the paired coordinators
                if tokens[i2] == '[c]':
                    return [], [], [], []
        else:
            if pred_tags[i2] == 'B-before':
                if len(tmp_tokens) > 0:
                    conjuncts.append(tmp_tokens)
                    cj_ctags.append(tmp_ctags)
                    cj_offsets.append(tmp_offsets)
                    tmp_tokens = []
                    tmp_ctags = []
                    tmp_offsets = []
                tmp_tokens.append(tokens[i2])
                tmp_ctags.append(ctags[i2])
                tmp_offsets.append(offsets[i2])
            elif pred_tags[i2] == 'C':
                if len(tmp_tokens) > 0:
                    conjuncts.append(tmp_tokens)
                    cj_ctags.append(tmp_ctags)
                    cj_offsets.append(tmp_offsets)
                    tmp_tokens = []
                    tmp_ctags = []
                    tmp_offsets = []
            elif pred_tags[i2] == 'O':
                if len(tmp_tokens) > 0:
                    conjuncts.append(tmp_tokens)
                    cj_ctags.append(tmp_ctags)
                    cj_offsets.append(tmp_offsets)
                end_id = i2
                break
            else:
                tmp_tokens.append(tokens[i2])
                tmp_ctags.append(ctags[i2])
                tmp_offsets.append(offsets[i2])

    end_tokens = []
    end_ctags = []
    end_offsets = []
    #ending
    for i3 in range(end_id, len(tokens)):
        end_tokens.append(tokens[i3])
        end_ctags.append(ctags[i3])
        end_offsets.append(offsets[i3])

    #generate sub-sentences
    finished = True
    tmp_sub = []
    tmp_c = []
    tmp_o = []
    for n in range(len(conjuncts)):
        cur_conjunct = conjuncts[n]
        cur_ctags = cj_ctags[n]
        cur_offsets = cj_offsets[n]
        for t1 in range(len(bg_tokens)):
            tmp_sub.append(bg_tokens[t1])
            tmp_c.append(bg_ctags[t1])
            tmp_o.append(bg_offsets[t1])
            if bg_ctags[t1] == 'c-C':
                finished = False

        for t2 in range(len(cur_conjunct)):
            tmp_sub.append(cur_conjunct[t2])
            tmp_c.append(cur_ctags[t2])
            tmp_o.append(cur_offsets[t2])
            if cur_ctags[t2] == 'c-C':
                finished = False

        for t3 in range(len(end_tokens)):
            tmp_sub.append(end_tokens[t3])
            tmp_c.append(end_ctags[t3])
            tmp_o.append(end_offsets[t3])
            if end_ctags[t3] == 'c-C':
                finished = False

        if finished == True:
            finished_subs.append(tmp_sub)
            finished_offsets.append(tmp_o)
        else:
            unfinished_subs.append(tmp_sub)
            unfinished_ctags.append(tmp_c)
            unfinished_offsets.append(tmp_o)

        finished = True
        tmp_sub = []
        tmp_c = []
        tmp_o = []

    return finished_subs, finished_offsets, unfinished_subs, unfinished_ctags, unfinished_offsets


def process_data(data_path):
    df = pd.read_csv(data_path, encoding="latin-1")
    #df.loc[:, "Sentence #"] = df["Sentence #"].fillna(method="ffill")

    enc_ctag = preprocessing.LabelEncoder()
    enc_tag = preprocessing.LabelEncoder()

    df.loc[:, "c-Tag"] = enc_ctag.fit_transform(df["c-Tag"])
    df.loc[:, "Tag"] = enc_tag.fit_transform(df["Tag"])

    sentences = df.groupby("Sentence #")["Text"].apply(list).values
    ctag = df.groupby("Sentence #")["c-Tag"].apply(list).values
    tag = df.groupby("Sentence #")["Tag"].apply(list).values
    offsets = df.groupby("Sentence #")["Offset"].apply(list).values

    return sentences, ctag, tag, enc_ctag, enc_tag, offsets



if __name__ == "__main__":

    test_model_path = "saved_models/model4_2.bin"
    #outF = open("../../predictions/test_crf_allaug4_1.txt", "a")

    meta_data = joblib.load("saved_models/meta_2.bin")
    enc_train_ctag = meta_data["enc_train_ctag"]
    enc_train_tag = meta_data["enc_train_tag"]

    num_ctag = len(list(enc_train_ctag.classes_))
    num_tag = len(list(enc_train_tag.classes_))

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = EntityModel(num_ctag=num_ctag, num_tag=num_tag)
    model.load_state_dict(torch.load(test_model_path))
    model.to(device)


    PMIDs = ['29108061', '28341048', '24879756', '24634129', '20963633', '20373023', '20059931', '19763868', '19620661',
             '19181764', '22100596', '22436149', '22443815', '22445144', '23508028', '26440326']

    #PMIDs = ['28341048']
    #outFolder = "../splitted_PubMed/"
    #inFolder = "../predictions/"

    fieldnames = ['Sentence #', 'Text', 'c-Tag', 'Tag', 'Offset']


    for pmid in PMIDs:

        path1 = "../splits/" + pmid
        if not os.path.exists(path1):
            os.makedirs(path1)

        path2 = "../splitted_PubMed/" + pmid
        if not os.path.exists(path2):
            os.makedirs(path2)

        print(pmid)
        print()

        inF = open("../../PubMed_abstracts/" + pmid + ".txt", "r")
        ori_Lines = inF.readlines()

        in_sentences = nltk.sent_tokenize(ori_Lines[0])
        num_sen = len(in_sentences)
        inF.close()

        for file_cnt in range(1, num_sen+1):
            round = 0
            while (round >= 0 and round < 10):
                test_file = path1 + "/" + str(file_cnt) + "_pred_{}.csv".format(round)
                if round == 0:
                    test_file = "../predictions/" + pmid + "/" + str(file_cnt) + "_pred_{}.csv".format(round)

                # contain all finished sentences
                outF1 = open(path2 + "/" + str(file_cnt) + "_f.txt", "a")
                outF3 = open(path2 + "/" + str(file_cnt) + "_f_offset.txt", "a")
                outF2 = open(path1 + "/" + str(file_cnt) + "_pred_{}.csv".format(round+1), "w")

                writer = csv.DictWriter(outF2, fieldnames=fieldnames)
                writer.writeheader()

                test_sentences, test_ctag, test_tag, enc_test_ctag, enc_test_tag, offsets_lst = process_data(test_file)
                test_dataset = dataset.EntityDataset(
                    texts=test_sentences, ctags=test_ctag, tags=test_tag
                )

                correct_preds = 0
                total_preds = 0
                total_correct = 0

                f_cnt = 1
                uf_cnt = 1

                ori_tokens = nltk.word_tokenize(in_sentences[file_cnt-1])
                ori_sentence_length = len(ori_tokens)
                has_longer = False

                """
                offsets_lst = []
                testF = open(test_file, "r")
                Lines = csv.reader(testF)
                is_fst = True
                sen_cnt = 1
                tmp_offsets = []
                for l in Lines:
                    # skip the fieldnames
                    if is_fst is True:
                        is_fst = False
                        continue
                    if str(sen_cnt) == l[0].split(" ")[1]:
                        tmp_offsets.append(l[4])
                    else:
                        sen_cnt += 1
                        offsets_lst.append(tmp_offsets)
                        tmp_offsets = []
                        tmp_offsets.append(l[4])
                offsets_lst.append(tmp_offsets)
                # print(offsets_lst)
                testF.close()
                """


                with torch.no_grad():
                    for j in range(len(test_dataset)):
                        try:
                            data = test_dataset[j]
                            tokenized_sentence_ids = data["ids"]
                            c_tags = data["target_ctag"]
                            target_tags = data["target_tag"]
                            tokenized_sentence = config.TOKENIZER.convert_ids_to_tokens(tokenized_sentence_ids)

                            for k, v in data.items():
                                data[k] = v.to(device).unsqueeze(0)
                            tag, _ = model(**data)

                            #raw_tags = enc_train_tag.inverse_transform(tag.argmax(2).cpu().numpy().reshape(-1))[:len(tokenized_sentence_ids)]
                            raw_tags = enc_train_tag.inverse_transform(tag[0])[:len(tokenized_sentence_ids)]
                            #print(raw_tags)
                            raw_target_ctags = enc_train_ctag.inverse_transform(c_tags)[
                                              :len(tokenized_sentence_ids)]
                            raw_target_tags = enc_train_tag.inverse_transform(target_tags)[
                                   :len(tokenized_sentence_ids)]


                            tokens, labels = getCleanLabels(tokenized_sentence, raw_tags)
                            _, target_clabels = getCleanLabels(tokenized_sentence, raw_target_ctags)


                            if (len(tokens) != len(offsets_lst[j])):
                                print(len(tokens))
                                print(tokens)
                                print(len(offsets_lst[j]))
                                print(offsets_lst[j])
                                print("-----skipped-----")
                                continue

                            f_subs, f_offsets, uf_subs, uf_ctags, uf_offsets = splitter(tokens, target_clabels, labels, offsets_lst[j])

                            #print(f_subs)
                            #print(uf_subs)

                            #finished sub-sentences
                            for n in range(len(f_subs)):
                                sub = f_subs[n]
                                ofs = f_offsets[n]
                                if len(sub) >= ori_sentence_length:
                                    has_longer = True
                                    continue
                                for m in range(len(sub)):
                                    outF1.write(sub[m])
                                    outF1.write(" ")
                                    outF3.write(ofs[m])
                                    outF3.write(" ")
                                outF1.write("\n")
                                outF3.write("\n")

                            #unfinished sub-sentences
                            for k in range(len(uf_subs)):
                                sstr = "Sentence: " + str(uf_cnt)
                                fst = True
                                marking = False
                                ufsub = uf_subs[k]
                                ufctag = uf_ctags[k]
                                ufofs = uf_offsets[k]
                                for m2 in range(len(ufsub)):
                                    if ufctag[m2] == 'c-C':
                                        if fst is True:
                                            fst = False
                                            marking = True
                                            writer.writerow({'Sentence #': sstr, 'Text': '[C]', 'c-Tag': 'c-C', 'Tag': 'C', 'Offset': '[-1, -1]'})
                                    else:
                                        if marking is True:
                                            marking = False
                                            writer.writerow({'Sentence #': sstr, 'Text': '[C]', 'c-Tag': 'c-C', 'Tag': 'C', 'Offset': '[-1, -1]'})

                                    writer.writerow({'Sentence #': sstr, 'Text': ufsub[m2], 'c-Tag': ufctag[m2], 'Tag': 'O', 'Offset': ufofs[m2]})
                                uf_cnt += 1

                        except Exception:
                            print(Exception)
                            pass

                outF1.close()
                outF2.close()

                print("f_cnt:")
                print(f_cnt)
                print("uf_cnt:")
                print(uf_cnt)


                if uf_cnt == 1 or has_longer == True:
                    round = -1
                # wrong & misleading predicted labels
                elif uf_cnt > 100:
                    round = -1
                    outF1 = open(path2 + "/" + str(file_cnt) + "_f.txt", "w")
                    outF1.close()
                else:
                    round += 1


